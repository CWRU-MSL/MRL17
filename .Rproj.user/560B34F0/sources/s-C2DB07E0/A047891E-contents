---
title: 'CWRU DSCI351-451: EDA: Semester Project 3-4'
author: "Amit K Verma"
date: "April 30, 2015"
output:
  html_document:
    toc: true
    font-size: 10em
    self_contained: true
  beamer_presentation:
    toc: true
  pdf_document:
    toc: true
---

<!--
# Script Name: 1503-Verma-DSCI351-451-SemProj3-4.Rmd
# Purpose: Defining the problem for Semester Project
# Authors: Amit K. Verma
# License: Creative Commons Attribution-ShareAlike 4.0 International License.
##########
# Latest Changelog Entires:
# v0.02.04 - 1504-30 - Amit Verma working on SemProj3-4 - completed 
##########

# Rmd code goes below the comment marker!
-->

##### License: Creative Commons Attribution-ShareAlike 4.0 International License.

### Title: Study of aluminum graded composites using data-analytics to probe into material measures governing its performance

 <center>![Schematic](../figs/schematic.jpg)

## Question

 To build a model which can correlate the performance from a known microstructure

##### Intial Frame Work

  - Known Microstructure
  - Only one aspect of performance - Strength/YS/Hardness

## Data-Sets

  - Three different data sets, each containing data from different instrument
    - First, Hardness measurements, specific to an area equal to the size of indent
    - Second, EDS compositional data, specific to an area equal to the size of interaction volume
    - Third, EBSD texture data, specific to an area scanned over in a single map
    
## Target Data Structure

  - Requirement of Target Data Structure is to enable navigation through all data
  - To enable this, we put the data from all different data-sets in a single reference system (described in earlier presentation & also mentioned in DataBook)
  - Once the data is in single reference system, we can merge, visualize and correlate 
  
## Exploratory data analysis & Data visualization

 
```{r,echo=FALSE}
# function to add p values and R values to the upper half of pair plots 
panel.cor <- function(x, y, digits = 2, cex.cor, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  # correlation coefficient
  r <- cor(x,y)
  txt <- format(c(r, 0.123456789), digits = digits)[1]
  txt <- paste("r= ", txt, sep = "")
  text(0.5, 0.6, txt)
 
  # p-value calculation
  p <- cor.test(x, y)$p.value
  txt2 <- format(c(p, 0.123456789), digits = digits)[1]
  txt2 <- paste("p= ", txt2, sep = "")
  if(p<0.01) txt2 <- paste("p= ", "<0.01", sep = "")
  text(0.5, 0.4, txt2)
}
```


```{r, echo=FALSE}
# reading Hardness Data of Sample1
setwd("../data/Hardness")
df <- read.csv("Sample1_Hardness.csv", header = TRUE, stringsAsFactors=FALSE)
df$Location.Z <- df$disp.ref.z + df$dz*df$Location.Z
df$Location.X <- df$disp.ref.x + df$dx*df$Location.X
drops <- c("disp.ref.z","disp.ref.x", "dz", "dx")
strength <- df[,!(names(df) %in% drops)]
```


```{r, echo=FALSE}
# reading Hardness Data of Sample3
setwd("../data/Hardness")
df <- read.csv("Sample3_Hardness.csv", header = TRUE, stringsAsFactors=FALSE)
df$Location.Z <- df$disp.ref.z + df$dz*df$Location.Z
df$Location.X <- df$disp.ref.x + df$dx*df$Location.X
drops <- c("disp.ref.z","disp.ref.x", "dz", "dx")
strength2 <- df[,!(names(df) %in% drops)]
```


```{r, echo=FALSE}
# reading Composition Data of Sample1

setwd("../data/EDS")
df <- read.csv("Sample1_EDS2.csv", header = TRUE, stringsAsFactors=FALSE)
df$Location.Z <- df$disp.ref.z + df$dz*df$Location.Z
df$Location.X <- df$disp.ref.x + df$dx*df$Location.X
drops <- c("disp.ref.z","disp.ref.x", "dz", "dx")
composition <- df[,!(names(df) %in% drops)]
l <- length(composition$Al)
sum <- 0

# 'for' loop to normalize the results to 100%

for (i in 1:l)
  {sum <- composition$Al[i] + composition$Cu[i] + composition$Mg[i] + composition$Zn[i]
   composition$Al[i] <- (composition$Al[i]/sum)*100
   composition$Cu[i] <- (composition$Cu[i]/sum)*100
   composition$Mg[i] <- (composition$Mg[i]/sum)*100
   composition$Zn[i] <- (composition$Zn[i]/sum)*100}

```

 
```{r, echo=FALSE}
# reading Composition Data of Sample3

setwd("../data/EDS")
df <- read.csv("Sample3_EDS.csv", header = TRUE, stringsAsFactors=FALSE)
df$Location.Z <- df$disp.ref.z + df$dz*df$Location.Z
df$Location.X <- df$disp.ref.x + df$dx*df$Location.X
drops <- c("disp.ref.z","disp.ref.x", "dz", "dx")
composition2 <- df[,!(names(df) %in% drops)]
l <- length(composition2$Al)
sum <- 0

# 'for' loop to normalize the results to 100%

for (i in 1:l)
  {sum <- composition2$Al[i] + composition2$Cu[i] + composition2$Mg[i] + composition2$Zn[i]
   composition2$Al[i] <- (composition2$Al[i]/sum)*100
   composition2$Cu[i] <- (composition2$Cu[i]/sum)*100
   composition2$Mg[i] <- (composition2$Mg[i]/sum)*100
   composition2$Zn[i] <- (composition2$Zn[i]/sum)*100}

```


#### Training Data - Used for generating correlation

  - r = correlation coefficients 
  - p = p values for the correlation test

```{r, echo=FALSE,warning=FALSE}

df <- merge(strength, composition, by = c("Location.X","Location.Z"))
# Combining two data-sets to visualize together

data <- subset(df, select = c(Location.X, Location.Z, Hardness, Al, Zn, Mg, Cu))

# Changing to atomic percentage

N <- 6.022e23
l <- length(data$Al)
columns <- c("Alat","Mgat","Znat", "Cuat")
data[,columns] <- NA

for (i in 1:l)
  {data$Alat[i] <- data$Al[i]*N/27
  data$Znat[i] <- data$Zn[i]*N/65.4
  data$Mgat[i] <- data$Mg[i]*N/24.3
  data$Cuat[i] <- data$Cu[i]*N/63.5}


for (i in 1:l)
  {sum <- data$Alat[i] + data$Cuat[i] + data$Mgat[i] + data$Znat[i]
   data$Alat[i] <- (data$Alat[i]/sum)*100
   data$Cuat[i] <- (data$Cuat[i]/sum)*100
   data$Mgat[i] <- (data$Mgat[i]/sum)*100
   data$Znat[i] <- (data$Znat[i]/sum)*100}

# Changing column names

colnames(data) <- c("X.mm", "Z.mm", "Hardness", "Al.wt", "Zn.wt", "Mg.wt", "Cu.wt", "Al.at", "Mg.at", "Zn.at", "Cu.at")


# pair plot of training data-set
a <- c(1,2,3,8,9,10,11)
pairs(data[,a], upper.panel = panel.cor)

```

#sgSEM quantification

```{r}

library(sgSEM)
b <- c(2,3,8,9,10,11)
sgSEM1 <- data[,b]
ans <- sgSEM(sgSEM1)
```

![](../figs/MRplotat.jpg)

  - From the data, we can see that Mg & Zn are highly correlated with Hardness Value
  - To define H(x,z) = f(composition), we can use Mg & Zn; and can leave out Al & Cu
  - Using sg-SEM principle 1, we got the best fit between Hardness~Zn, &, Hardness~Mg, which uses Adj.R-square values for ranking
  
## Interpret results

 - H(x,z) = f (2 variables)
 - H(x,z) = f (Zn, Mg)
 - 5 Variables: H(x,z), C(x,z) - [ Al, Zn, Mg, Cu]
 - X direction has uniform composition
 - Z direction has varying composition
 
 - Using sgSEM
     - H(z) = a X log(Mg) + b X exp(Zn) + c
  
  

```{r}
#combining different forms of equations and checking p-values 

fit <- with(data, lm(Hardness ~ I(Mg.at^0.5) + exp(Zn.at)))
summary(fit)

fit <- with(data, lm(Hardness ~ I(Mg.at) + I(Zn.at)))
summary(fit)

fit <- with(data, lm(Hardness ~ exp(Zn.at)))
summary(fit)

fit <- with(data, lm(Hardness ~ I(Mg.at^0.5)))
summary(fit)

fit <- with(data, lm(Hardness ~ I(Mg.at) + I(Zn.at) + I(Mg.at*Zn.at)))
summary(fit)

```

  
## Challenge results

  - Different sample was taken with different data density to check the above correlation
  - Data is approximately half in comparison of data used for generating correlation, so the ratio is 66:34 
  - I've taken data at 3 different Z-Location as compared to data at 6 different Z-Location for building the correlation, but 22 times at each Z as to confirm the variance interval, and avoid outliers
  - Besides, one or two outliers, which I removed very cautiously (compare the two pair plots, only two points removed out of total of 66), we can see that the data intervals are pretty sharp (Location.Z Vs Al,Mg, or Zn)
  - For challenging, I've evaluated the 'lm' fitted models against actual values and presented using ggplot2 in results
  
      - r = correlation coefficients 
      - p = p values for the correlation test
  
```{r, echo=FALSE}
df <- merge(strength2, composition2, by = c("Location.X","Location.Z"))

# pair plot without removing any outliers

b <- c(1,2,10,21,23,24,22)
pairs(df[,b], upper.panel = panel.cor)

#removing outliers by checking through above pair plot

df <- df[-c(3, 60), ]

b <- c(1,2,10,21,23,24,22)
pairs(df[,b], upper.panel = panel.cor)

data2 <- subset(df, select = c(Location.X, Location.Z, Hardness, Al, Zn, Mg, Cu))

```

```{r, echo=FALSE,warning=FALSE}
# Changing to atomic percentage

N <- 6.022e23
l <- length(data2$Al)
columns <- c("Alat","Mgat","Znat", "Cuat")
data2[,columns] <- NA

for (i in 1:l)
  {data2$Alat[i] <- data2$Al[i]*N/27
  data2$Znat[i] <- data2$Zn[i]*N/65.4
  data2$Mgat[i] <- data2$Mg[i]*N/24.3
  data2$Cuat[i] <- data2$Cu[i]*N/63.5}


for (i in 1:l)
  {sum <- data2$Alat[i] + data2$Cuat[i] + data2$Mgat[i] + data2$Znat[i]
   data2$Alat[i] <- (data2$Alat[i]/sum)*100
   data2$Cuat[i] <- (data2$Cuat[i]/sum)*100
   data2$Mgat[i] <- (data2$Mgat[i]/sum)*100
   data2$Znat[i] <- (data2$Znat[i]/sum)*100}

colnames(data2) <- c("X.mm", "Z.mm", "Hardness", "Al.wt", "Zn.wt", "Mg.wt", "Cu.wt", "Al.at", "Mg.at", "Zn.at", "Cu.at")
```
  
## Synthesize/Write-up Results

  - (Predicted Value) coming from H(z) = 247.11928 - 28.82181(sqrt(Mg)) + 0.23847(exp(Zn))

  - Predicted Values are well with in the range of actual values, besides the first interval, where we have all the predicted value at bottom extreme


```{r,echo=FALSE,warning=FALSE}

l <- length(data2$Zn.at)

# Adding columns to existing data frame to add predicted values

columns <- c("H4")
data2[,columns] <- NA

# calculating predicted values using equations coming from 'lm'

for(i in 1:l){
  data2$H4[i] <- 183.76144 + 0.38144*exp(data2$Zn.at[i])}

# comparing the values using ggplot2

plot(data2$Z.mm, data2$Hardness, pch=16, col="blue", xlab="Z(mm)", ylab="Hardness (VHN)", cex=1.5)
legend(4,205, c("Actual","Predicted"), pch=c(16,20), bty="o", col=c("blue","red"))
par(new=TRUE)
plot(data2$Z.mm, data2$H4, pch=20, col="red", xlab="", ylab="", xaxt="n", yaxt="n", cex=1.25)

plot(data2$Hardness, data2$H4, pch=16, xlab="Actual HV", ylab="Predicted HV")
par(new=TRUE)
abline(a=0, b=1)

```


  - Predicted Values fits well, but with condense range in values, while the actual values shows high variation 
  
  - EBSD couldn't be included in this report, because detector/machine was down for whole month, also due to availability of machine at odd times 
  
#### Databook and Codebook

  - Submitted separately in different files
  
#### Datasets and structures
 
  - Already defined and can be checked with head/str(strength/composition) for datasets
 
<!--
# Keep a complete change log history at bottom of file.
# Complete Change Log History
# v0.00.00 - 1405-07 - Nick Wheeler made the blank script
# v0.00.01 - 1503-05 - Amit Verma started working on Semester Project
# v0.01.01 - 1503-24 - Amit Verma took SemProject1 file and started working on SemProj2
# v0.01.05 - 1504-01 - Amit Verma working on SemProj2 - added data visualization graphs
# v0.02.01 - 1504-21 - Amit Verma started working on SemProj3-4
# v0.02.02 - 1504-22 - Amit Verma working on SemProj3-4
# v0.02.03 - 1504-29 - Amit Verma working on SemProj3-4 - made a copy for making changes for the submission
# v0.02.04 - 1504-30 - Amit Verma working on SemProj3-4 - completed
-->